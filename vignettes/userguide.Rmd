---
title: Saving objects to artifacts and back again
author:
- name: Aaron Lun
  email: infinite.monkeys.with.keyboards@gmail.com
package: alabaster.base
date: "Revised: November 16, 2023"
output:
  BiocStyle::html_document
vignette: >
  %\VignetteIndexEntry{Saving and loading artifacts}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, echo=FALSE}
library(BiocStyle)
self <- Biocpkg("alabaster.base");
knitr::opts_chunk$set(error=FALSE, warning=FALSE, message=FALSE)
```

# Introduction

The `r self` package (and its family) implements methods to save common Bioconductor objects to file artifacts and load them back into R.
This aims to provide a functional equivalent to RDS-based serialization that is:

- More stable to changes in the class definition.
  Such changes would typically require costly `updateObject()` operations at best, or invalidate RDS files at worst.
- More interoperable with other analysis frameworks.
  All artifacts are saved in standard formats (e.g., CSV, HDF5) and can be easily parsed by applications in other languages.
- More modular, with each object split into multiple artifacts. 
  This enables parts of the object to be loaded into memory according to each application's needs.
  Parts can also be updated cheaply on disk without rewriting all files.

# Quick start

To demonstrate, let's mock up a `DataFrame` object from the `r Biocpkg("S4Vectors")` package.

```{r}
library(S4Vectors)
df <- DataFrame(X=1:10, Y=letters[1:10])
df
```

We'll save this `DataFrame` to a directory:

```{r}
tmp <- tempfile()
library(alabaster.base)
saveObject(df, tmp)
```

And read it back in:

```{r}
readObject(tmp)
```

# Class-specific methods

Each class implements a saving and reading method for use by the _alabaster_ framework.
The saving method (for the `saveObject()` generic) will save the object to one or more files, given a path to a directory:

```{r}
tmp <- tempfile()
saveObject(df, tmp)
list.files(tmp, recursive=TRUE)
```

Conversely, the reading function will - as the name suggests - load the object back into memory, given the path to its directory. 
The correct loading function for each class is automatically called by the `readObject()` function:

```{r}
readObject(tmp)
```

`r self` itself supports a small set of classes from the `r Biocpkg("S4Vectors")` packages;
support for additional classes can be found in other packages like `r Biocpkg("alabaster.ranges")` and `r Biocpkg("alabaster.se")`.
Third-party developers can also add support for their own classes by defining new methods, see the [_Extensions_](extensions.html) vignette for details.

# Operating on directories

Users can move freely rename or relocate directories and `readObject()` function will still work.
For example, we can easily copy the entire directory to a new file system and everything will still be correctly referenced.
The simplest way to share objects is to just `zip` or `tar` the staging directory for _ad hoc_ distribution.
For more serious applications, `r self` can be used in conjunction with centralized file storage systems - 
see the [_Applications_](applications.html) vignette for more details.

```{r}
tmp <- tempfile()
saveObject(df, tmp)

tmp2 <- tempfile()
file.rename(tmp, tmp2)
readObject(tmp2)
```

That said, it is unwise to manipulate the files inside the directory created by `saveObject()`.
Reading functions will usually depend on specific file names or subdirectory structures within the directory, and fiddling with them may cause unexpected results.

```{r}
# Creating a nested DF to be a little spicy:
df2 <- DataFrame(Z=factor(1:5), AA=I(DataFrame(B=runif(5), C=rnorm(5))))
tmp <- tempfile()
meta2 <- saveObject(df2, tmp)

list.files(tmp, recursive=TRUE)
readObject(file.path(tmp, "other_columns/1"))
```

# Extending to new classes

The _alabaster_ framework is easily extended to new classes by:

1. Writing a method for `saveObject()`.
   This should accept an instance of the object and a path to a directory, and save the contents of the object inside the directory.
   It should also produce an `OBJECT` file that specifies the type of the object, e.g., `data_frame`, `hdf5_sparse_matrix`.
2. Writing a function for `readObject()` and registering it with `registerReadObjectFunction()` (or, for core Bioconductor classes, by requesting a change to the default registry in `r self`).
   This should accept a path to a directory and read its contents to reconstruct the object.
   The registered type should be the same as that used in the `OBJECT` file.
3. Writing a function for `validateObject()` and registering it with `registervalidateObjectFunction()`.
   This should accept a path to a directory and read its contents to determine if it is a valid on-disk representation.
   The registered type should be the same as that used in the `OBJECT` file.
   - (optional) Devleopers can alternatively formalize the on-disk representation by adding a specification to the [**takane**](https://github.com/ArtifactDB/takane) repository.
   This aims to provide C++-based validators for each representation, allowing us to enforce consistency across multiple languages (e.g., Python).
   Any **takane** validator is automatically used by `validateObject()` so no registration is required.

To illustrate, let's extend _alabaster_ to the `dgTMatrix` from the `r CRANpkg("Matrix")` package.
First, the saving method:

```{r}
library(Matrix)
setMethod("saveObject", "dgTMatrix", function(x, path, ...) {
    # Create a directory to stash our contents.
    dir.create(path)

    # Saving a DataFrame with the triplet data.
    df <- DataFrame(i = x@i, j = x@j, x = x@x)
    write.csv(df, file.path(path, "matrix.csv"), row.names=FALSE)

    # Adding some more information.
    write(dim(x), file=file.path(path, "dimensions.txt"), ncol=1)

    # Creating an object file.
    write("triplet_sparse_matrix", file=file.path(path, "OBJECT"))
})
```

And now the reading method:

```{r}
readSparseTripletMatrix <- function(path) {
    df <- read.table(file.path(path, "matrix.csv"), header=TRUE, sep=",")
    dims <- readLines(file.path(path, "dimensions.txt"))
    sparseMatrix(
         i=df$i + 1L, 
         j=df$j + 1L, 
         x=df$x, 
         dims=as.integer(dims),
         repr="T"
    )
}

registerReadObjectFunction("triplet_sparse_matrix", readSparseTripletMatrix)
```

And running them:

```{r}
x <- sparseMatrix(
    i=c(1,2,3,5,6), 
    j=c(3,6,1,3,8), 
    x=runif(5), 
    dims=c(10, 10), 
    repr="T"
)
x

tmp <- tempfile()
saveObject(x, tmp, "test")
list.files(tmp, recursive=TRUE)
readObject(tmp)
```

For more complex objects that are composed of multiple smaller "child" objects, developers should consider saving each of their children in subdirectories of `path`.
This can be achieved by calling `altSaveObject()` and `altReadObject()` in the saving and loading functions, respectively.

# Creating applications

Developers can also create applications that customize the machinery of the _alabaster_ framework for specific needs.
For example, we might want to deposit some metadata for each saved object - say, the authoring user.
This is achieved by creating an application-specific saving generic:

```{r}
setGeneric("appSaveObject", function(x, path, ...) {
    ans <- standardGeneric("appSaveObject")

    # File names with leading underscores are reserved for application-specific
    # use, so they won't clash with anything produced by stageObject. 
    metapath <- file.path(path, "_metadata.json")
    write(jsonlite::toJSON(ans, auto_unbox=TRUE), file=metapath)
})

setMethod("appSaveObject", "ANY", function(x, path, ...) {
    saveObject(x, path, ...) # does the real work
    list(authors=I(Sys.info()[["user"]])) # adds the desired metadata
})
```

We can specialize the behavior for specific classes like `DataFrame`s:

```{r}
setMethod("appSaveObject", "DFrame", function(x, path, child=FALSE, ...) {
    ans <- callNextMethod()
    ans$columns <- I(colnames(x))
    ans
})
```

Applications can then override any `altSaveObject()` calls with their generic.
This ensures that the customizations are applied to all child objects, such as the nested `DataFrame` below.

```{r}
df2 <- DataFrame(Z=factor(1:5), AA=I(DataFrame(B=runif(5), C=rnorm(5))))

old <- altSaveObjectFunction(appSaveObject)
tmp <- tempfile()
altSaveObject(df2, tmp)

# Both the parent and child DataFrames have metadata.
cat(readLines(file.path(tmp, "_metadata.json")), sep="\n")
cat(readLines(file.path(tmp, "other_columns/1/_metadata.json")), sep="\n")

# Unsetting the override for other applications.
altSaveObjectFunction(old) 
```

The reading function can be similarly overridden, e.g., to do something with the metadata that we just added.

```{r}
appReadObject <- function(path, ...) {
    type <- readLines(file.path(path, "OBJECT"))

    # Adding a custom message based on the type and its metadata.
    meta <- jsonlite::fromJSON(file.path(path, "_metadata.json"))
    cat("I am a ", type, " created by ", meta$authors[1], ".\n", sep="")
    if (type == "data_frame") {
        all.cols <- paste(meta$columns, collapse=", ")
        cat("I have the following columns: ", all.cols, ".\n", sep="")
    }

    readObject(path, type=type, ...)
}

old <- altReadObjectFunction(appReadObject)
altReadObject(tmp)

# Unsetting the override for other applications.
altReadObjectFunction(old) 
```

# Session information {-}

```{r}
sessionInfo()
```

